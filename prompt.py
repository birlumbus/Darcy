import sys
import os

base_dir = os.path.dirname(__file__)
prompting_path = os.path.join(base_dir, 'src', 'scripts')
sys.path.append(prompting_path)

from prompting import prompt_gpt2medium as medium
from prompting import prompt_gpt2large as large
from prompting import prompt_gptj6b as gptj6b

from evaluation.calculate_perplexity import calculate_perplexity


model_categories = {
    "medium": {
        "category": medium,
        "models": {
            "0": "gpt2-medium",
            "1": "./models/darcy-gpt2-medium-1",
            "1.1": "./models/darcy-gpt2-medium-1.1",
            "1.2": "./models/darcy-gpt2-medium-1.2",
            "2": "./models/darcy-gpt2-medium-2",
            "2.1": "./models/darcy-gpt2-medium-2.1",
            "2.2": "./models/darcy-gpt2-medium-2.2"
        }
    },
    "large": {
        "category": large,
        "models": {
            "0": "gpt2-large",
            "1": "./models/darcy-gpt2-large-1",
            "1.1": "./models/darcy-gpt2-large-1.1",
            "1.2": "./models/darcy-gpt2-large-1.2",
            "2": "./models/darcy-gpt2-large-2",
            "2.1": "./models/darcy-gpt2-large-2.1",
            "2.2": "./models/darcy-gpt2-large-2.2"
        }
    }
    # "6b": {
    #     "category": gptj6b,
    #     "models": {
    #         "0": "EleutherAI/gpt-j-6B",
    #         "1": "./models/darcy-gptj-6b-1",
    #         "2": "./models/darcy-gptj-6b-2",
    #         "2.1": "./models/darcy-gptj-6b-2.1"
    #     }
    # }
}


def load_models_for_category(category):
    """
    Uses the category associated with a category to load models.

    Parameters:
        category (str): The category name (e.g. "medium").

    Returns:
        dict: Mapping of model IDs to (model, tokenizer) tuples.
    """
    cat_info = model_categories[category]
    mod = cat_info["category"]
    paths = cat_info["models"]
    return mod.load_models(paths)


def parse_selection(selection):
    """
    Parse user selection and return a dict mapping category to list of model IDs.

    Acceptable formats:
      - "all"                -> all models from all categories
      - "medium"             -> all models in medium
      - "large:1"            -> only model "1" from large
      - "6b:all"             -> all models in 6b
      - Comma-separated values, e.g. "medium:1, 6b"
    """
    selection = selection.strip().lower()
    if selection == "all":
        result = {}
        for category, cat_info in model_categories.items():
            result[category] = list(cat_info["models"].keys())
        return result

    result = {}
    parts = selection.split(',')
    for part in parts:
        part = part.strip()
        if ':' in part:
            cat, mod = part.split(':', 1)
            cat = cat.strip()
            mod = mod.strip()
            if cat in model_categories:
                if mod == "all":
                    result[cat] = list(model_categories[cat]["models"].keys())
                else:
                    if mod in model_categories[cat]["models"]:
                        result.setdefault(cat, []).append(mod)
                    else:
                        print(f"Warning: Model '{mod}' not found in category '{cat}'.")
            else:
                print(f"Warning: Category '{cat}' not recognized.")
        else:
            cat = part
            if cat in model_categories:
                result[cat] = list(model_categories[cat]["models"].keys())
            else:
                print(f"Warning: Category '{cat}' not recognized.")
    return result


def run_tests_for_model(category, model_id, output_text, base_outputs, model, tokenizer):
    """
    Computes perplexity.

    Parameters:
        category (str): The model category.
        model_id (str): The model identifier.
        output_text (str): The text generated by the model.
        base_outputs (dict): Dictionary storing outputs from base models.
        model: The model instance.
        tokenizer: The tokenizer instance.

    Returns:
        str: A formatted string containing the perplexity result.
    """
    if not output_text or not output_text.strip():
        return "[[no output]]"

    # always calculate perplexity
    perplexity_val = calculate_perplexity(output_text, model, tokenizer)

    return f"\nPerplexity: {perplexity_val}\n"


def prompt_single_model(category, model_id, prompt_text, loaded_models, base_outputs=None):
    """
    Prompts a single model and returns its generated output along with perplexity evaluation.

    Parameters:
        category (str): The model category.
        model_id (str): The model identifier.
        prompt_text (str): The prompt text.
        loaded_models (dict): Pre-loaded models.
        base_outputs (dict, optional): Dictionary for storing base model outputs.

    Returns:
        str: The model output and perplexity evaluation.
    """
    base_outputs = base_outputs if base_outputs is not None else {}

    if model_id not in loaded_models:
        return f"From {category}-{model_id}:\nModel not found.\n\n"

    print(f"\nPROMPTING MODEL: {category}-{model_id}\n")
    model, tokenizer = loaded_models[model_id]
    mod = model_categories[category]["category"]

    # ensure a non-empty output is produced for the base model.
    if model_id == "0":
        attempts = 0
        output_text = mod.generate_text(prompt_text, model, tokenizer, max_length=256)
        while not (output_text and output_text.strip()):
            attempts += 1
            print(f"Base model produced no output. Prompting again... (attempt {attempts})")
            output_text = mod.generate_text(prompt_text, model, tokenizer, max_length=256)
            if attempts >= 10:
                print("\nWARNING: Maximum attempts reached. Proceeding with empty output.\n")
                break
    else:
        output_text = mod.generate_text(prompt_text, model, tokenizer, max_length=256)

    print()  # line break

    test_metrics = run_tests_for_model(category, model_id, output_text, base_outputs, model, tokenizer)
    model_output_info = f"From {category}-{model_id}:\n{output_text}\n{test_metrics}"
    return model_output_info


def process_category(category, prompt_text, base_outputs):
    """
    Processes all models in a category and returns their output.

    Parameters:
        category (str): The model category.
        prompt_text (str): The prompt text.
        base_outputs (dict): Dictionary for storing base model outputs.

    Returns:
        list: A list of outputs from each model.
    """
    loaded_models = load_models_for_category(category)
    model_outputs = [
        prompt_single_model(category, model_id, prompt_text, loaded_models, base_outputs)
        for model_id in model_categories[category]["models"]
    ]
    return model_outputs


def safe_save(output_file_path, result_text):
    """
    Attempts to save result_text to output_file_path. If an error occurs, prints an error message.

    Parameters:
        output_file_path (str): Path to the output file.
        result_text (str): Text to save.
    """
    try:
        with open(output_file_path, "w") as f:
            f.write(result_text)
        print(f"\nResults saved to {output_file_path}\n")
    except Exception as e:
        print(f"\nError saving results to file: {e}\n")


def file_mode(file_path, output_file_path):
    """
    Processes prompts from a file and evaluates model outputs. In case of an error during processing,
    attempts to save the progress made so far.

    Parameters:
        file_path (str): Path to the file containing prompts.
        output_file_path (str): Path to the file where results will be saved.
    """
    with open(file_path, "r") as f:
        prompt_list = [line.strip() for line in f if line.strip()]

    base_outputs = {}
    final_outputs = []

    try:
        # process each prompt.
        for prompt_text in prompt_list:
            print(f"\nNEW PROMPT:\n{prompt_text}\n")
            output_for_prompt = [f"Prompt text:\n{prompt_text}\n"]

            # process each category.
            for category in model_categories:
                output_for_prompt.extend(
                    process_category(category, prompt_text, base_outputs)
                )

            final_outputs.append("\n".join(output_for_prompt))
    except Exception as e:
        print(f"\nError encountered during processing: {e}\n")
        result_text = ("\n" + "-" * 80 + "\n\n").join(final_outputs)
        file_root, file_ext = os.path.splitext(output_file_path)
        incomplete_file_path = file_root + "_INCOMPLETE" + file_ext
        safe_save(incomplete_file_path, result_text)
        raise

    result_text = ("\n" + "-" * 80 + "\n\n").join(final_outputs)
    safe_save(output_file_path, result_text)


def interactive_mode():
    """
    Interactive mode prompts model(s) selected by the user and prints output to the console.
    Only perplexity is computed in this mode.
    """
    print("Available model categories and models:")
    for category, cat_info in model_categories.items():
        model_ids = ", ".join(cat_info["models"].keys())
        print(f"  {category}: {model_ids}")

    print("\nEnter your selection.")
    print("Examples:")
    print("  all")
    print("  medium")
    print("  large:1")
    print("  6b:2.1, large")
    print("  (ctrl-c to exit)")

    selected_dict = None
    while not selected_dict:
        selection_input = input("Your selection: ")
        selected_dict = parse_selection(selection_input)
        if not selected_dict:
            print("Invalid input. Try again.")

    prompt_text = input("\nEnter prompt:\n")
    print()

    base_outputs = {}
    outputs = {}

    for category, model_ids in selected_dict.items():
        loaded_models = load_models_for_category(category)
        for model_id in model_ids:
            outputs[f"{category}-{model_id}"] = prompt_single_model(
                category, model_id, prompt_text, loaded_models, base_outputs
            )
    print()


def main():
    # this number determines filename of in/output
    set_num = 7

    file_path = f"./src/evaluation_data/prompt_sets/prompt_set_{set_num}.txt"
    output_file_path = f"./src/evaluation_data/prompt_results/txt/prompt_results_{set_num}.txt"

    print("\nSelect mode:")
    print("  [s] Interactive mode (single prompt to model(s) of your choice)")
    print(f"  [f] File mode (submit prompts found in {file_path})")
    mode_choice = input("Your selection (default is s): ").strip().lower()

    if mode_choice == "f":
        file_mode(file_path, output_file_path)
    else:
        interactive_mode()

    print(f"\nAfter results are saved, run results_txt_to_json.py with arg {set_num}")
    print("Then, edit hard-coded filepath in bleu_rouge_meteor.py to analyze\n")


if __name__ == '__main__':
    main()
